{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikolaj\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from torchtext import vocab\n",
    "import pickle as pl\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Import model\n",
    "from Models.RNN import GRUNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "with open('data/English_encodings.pkl', 'rb') as f:\n",
    "    english_encodings,english_sentences,Paddings_en,Vocab_en = pl.load(f)\n",
    "with open('data/French_encodings.pkl', 'rb') as f:\n",
    "    french_encodings,french_sentences,Paddings_fr,Vocab_fr = pl.load(f)\n",
    "\n",
    "# Get the vocabulary size\n",
    "src_vocab_size = Vocab_fr.__len__()\n",
    "tgt_vocab_size = Vocab_en.__len__()\n",
    "src_padding_idx = Vocab_fr.__getitem__('<Pad>')\n",
    "tgt_padding_idx = Vocab_en.__getitem__('<Pad>')\n",
    "\n",
    "\n",
    "# Load Train,Vaildation and Test data\n",
    "with open('data/Train_data.pkl', 'rb') as f:\n",
    "    X_train,Y_train,src_padding_mask,tgt_padding_mask = pl.load(f)\n",
    "\n",
    "with open('data/Validation_data.pkl', 'rb') as f:\n",
    "    X_vali,Y_vali,src_padding_mask_vali = pl.load(f)\n",
    "\n",
    "with open('data/Test_data.pkl', 'rb') as f:\n",
    "    X_test,Y_test,src_padding_mask_test = pl.load(f)\n",
    "\n",
    "tgt_mask = th.triu(th.full((27, 27), float('-inf')), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 10000\n",
    "batch_size = 50\n",
    "# Define Training Function\n",
    "def train(model, optimizer, loss_fn, n_epochs, n_batches, X_train, Y_train):\n",
    "\n",
    "\n",
    "     # Check if CUDA/mps is available\n",
    "    if th.cuda.is_available():\n",
    "        device = th.device(\"cuda\")\n",
    "    elif th.backends.mps.is_available():\n",
    "        device = \"cpu\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    \n",
    "    # Move the model to the device\n",
    "    model.to(device)\n",
    "\n",
    "    # Predict token\n",
    "    predict_token = th.tensor([Vocab_fr.__getitem__('<Start>')],dtype=th.int32)\n",
    "\n",
    "    # Move the data to the device\n",
    "    X_train = X_train.to(device)\n",
    "    Y_train = Y_train.to(device)\n",
    "    predict_token = predict_token.to(device)\n",
    "\n",
    "    loss_batches = []\n",
    "    # Train for n_epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        loss_epoch = 0\n",
    "        for batch in range(n_batches):\n",
    "            model.to(device)\n",
    "\n",
    "            # Get the data\n",
    "\n",
    "            X_batch = X_train[batch]\n",
    "            Y_batch = Y_train[batch]\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred = model(X_batch,predict_token,device)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = 0 \n",
    "            for sample in range(batch_size):\n",
    "                loss += loss_fn(y_pred[sample], Y_batch[sample])\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            # Print the loss\n",
    "            loss_batches.append(loss.item())\n",
    "            loss_epoch += loss.item()\n",
    "\n",
    "        print('Epoch: %d, Batch: %d, Loss: %f' % (epoch, batch, loss_epoch/n_train))\n",
    "    return loss_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 199, Loss: 3.177315\n",
      "Epoch: 1, Batch: 199, Loss: 2.119747\n",
      "Epoch: 2, Batch: 199, Loss: 2.011071\n",
      "Epoch: 3, Batch: 199, Loss: 1.975379\n",
      "Epoch: 4, Batch: 199, Loss: 1.926619\n",
      "Epoch: 5, Batch: 199, Loss: 1.896645\n",
      "Epoch: 6, Batch: 199, Loss: 1.882116\n",
      "Epoch: 7, Batch: 199, Loss: 1.871537\n",
      "Epoch: 8, Batch: 199, Loss: 1.863516\n",
      "Epoch: 9, Batch: 199, Loss: 1.856687\n",
      "Epoch: 10, Batch: 199, Loss: 1.852032\n",
      "Epoch: 11, Batch: 199, Loss: 1.847987\n",
      "Epoch: 12, Batch: 199, Loss: 1.845449\n",
      "Epoch: 13, Batch: 199, Loss: 1.842615\n",
      "Epoch: 14, Batch: 199, Loss: 1.840647\n",
      "Epoch: 15, Batch: 199, Loss: 1.838642\n",
      "Epoch: 16, Batch: 199, Loss: 1.837027\n",
      "Epoch: 17, Batch: 199, Loss: 1.835567\n",
      "Epoch: 18, Batch: 199, Loss: 1.834379\n",
      "Epoch: 19, Batch: 199, Loss: 1.833399\n",
      "Epoch: 20, Batch: 199, Loss: 1.832366\n",
      "Epoch: 21, Batch: 199, Loss: 1.831525\n",
      "Epoch: 22, Batch: 199, Loss: 1.830869\n",
      "Epoch: 23, Batch: 199, Loss: 1.830263\n",
      "Epoch: 24, Batch: 199, Loss: 1.829708\n",
      "Epoch: 25, Batch: 199, Loss: 1.829092\n",
      "Epoch: 26, Batch: 199, Loss: 1.828646\n",
      "Epoch: 27, Batch: 199, Loss: 1.828272\n",
      "Epoch: 28, Batch: 199, Loss: 1.827982\n",
      "Epoch: 29, Batch: 199, Loss: 1.827560\n",
      "Epoch: 30, Batch: 199, Loss: 1.827089\n",
      "Epoch: 31, Batch: 199, Loss: 1.826781\n",
      "Epoch: 32, Batch: 199, Loss: 1.826448\n",
      "Epoch: 33, Batch: 199, Loss: 1.826300\n",
      "Epoch: 34, Batch: 199, Loss: 1.825990\n",
      "Epoch: 35, Batch: 199, Loss: 1.825803\n",
      "Epoch: 36, Batch: 199, Loss: 1.825575\n",
      "Epoch: 37, Batch: 199, Loss: 1.825348\n",
      "Epoch: 38, Batch: 199, Loss: 1.825218\n",
      "Epoch: 39, Batch: 199, Loss: 1.825195\n",
      "Epoch: 40, Batch: 199, Loss: 1.824901\n",
      "Epoch: 41, Batch: 199, Loss: 1.824865\n",
      "Epoch: 42, Batch: 199, Loss: 1.824653\n",
      "Epoch: 43, Batch: 199, Loss: 1.824623\n",
      "Epoch: 44, Batch: 199, Loss: 1.824409\n",
      "Epoch: 45, Batch: 199, Loss: 1.824297\n",
      "Epoch: 46, Batch: 199, Loss: 1.824199\n",
      "Epoch: 47, Batch: 199, Loss: 1.824062\n",
      "Epoch: 48, Batch: 199, Loss: 1.823951\n",
      "Epoch: 49, Batch: 199, Loss: 1.823928\n",
      "Epoch: 50, Batch: 199, Loss: 1.823876\n",
      "Epoch: 51, Batch: 199, Loss: 1.823761\n",
      "Epoch: 52, Batch: 199, Loss: 1.823581\n",
      "Epoch: 53, Batch: 199, Loss: 1.823518\n",
      "Epoch: 54, Batch: 199, Loss: 1.823497\n",
      "Epoch: 55, Batch: 199, Loss: 1.823402\n",
      "Epoch: 56, Batch: 199, Loss: 1.823372\n",
      "Epoch: 57, Batch: 199, Loss: 1.823486\n",
      "Epoch: 58, Batch: 199, Loss: 1.823361\n",
      "Epoch: 59, Batch: 199, Loss: 1.823491\n",
      "Epoch: 60, Batch: 199, Loss: 1.823369\n",
      "Epoch: 61, Batch: 199, Loss: 1.823249\n",
      "Epoch: 62, Batch: 199, Loss: 1.823361\n",
      "Epoch: 63, Batch: 199, Loss: 1.823152\n",
      "Epoch: 64, Batch: 199, Loss: 1.823039\n",
      "Epoch: 65, Batch: 199, Loss: 1.822959\n",
      "Epoch: 66, Batch: 199, Loss: 1.823029\n",
      "Epoch: 67, Batch: 199, Loss: 1.822880\n",
      "Epoch: 68, Batch: 199, Loss: 1.823164\n",
      "Epoch: 69, Batch: 199, Loss: 1.822905\n",
      "Epoch: 70, Batch: 199, Loss: 1.822836\n",
      "Epoch: 71, Batch: 199, Loss: 1.822833\n",
      "Epoch: 72, Batch: 199, Loss: 1.822784\n",
      "Epoch: 73, Batch: 199, Loss: 1.822803\n",
      "Epoch: 74, Batch: 199, Loss: 1.822926\n",
      "Epoch: 75, Batch: 199, Loss: 1.822817\n",
      "Epoch: 76, Batch: 199, Loss: 1.822681\n",
      "Epoch: 77, Batch: 199, Loss: 1.822763\n",
      "Epoch: 78, Batch: 199, Loss: 1.822627\n",
      "Epoch: 79, Batch: 199, Loss: 1.822608\n",
      "Epoch: 80, Batch: 199, Loss: 1.823076\n",
      "Epoch: 81, Batch: 199, Loss: 1.822604\n",
      "Epoch: 82, Batch: 199, Loss: 1.822519\n",
      "Epoch: 83, Batch: 199, Loss: 1.822550\n",
      "Epoch: 84, Batch: 199, Loss: 1.822433\n",
      "Epoch: 85, Batch: 199, Loss: 1.822508\n",
      "Epoch: 86, Batch: 199, Loss: 1.822533\n",
      "Epoch: 87, Batch: 199, Loss: 1.822484\n",
      "Epoch: 88, Batch: 199, Loss: 1.822456\n",
      "Epoch: 89, Batch: 199, Loss: 1.822415\n",
      "Epoch: 90, Batch: 199, Loss: 1.822516\n",
      "Epoch: 91, Batch: 199, Loss: 1.822410\n",
      "Epoch: 92, Batch: 199, Loss: 1.822383\n",
      "Epoch: 93, Batch: 199, Loss: 1.822337\n",
      "Epoch: 94, Batch: 199, Loss: 1.822309\n",
      "Epoch: 95, Batch: 199, Loss: 1.822324\n",
      "Epoch: 96, Batch: 199, Loss: 1.822347\n",
      "Epoch: 97, Batch: 199, Loss: 1.822377\n",
      "Epoch: 98, Batch: 199, Loss: 1.822308\n",
      "Epoch: 99, Batch: 199, Loss: 1.822437\n",
      "Epoch: 100, Batch: 199, Loss: 1.822281\n",
      "Epoch: 101, Batch: 199, Loss: 1.822252\n",
      "Epoch: 102, Batch: 199, Loss: 1.822545\n",
      "Epoch: 103, Batch: 199, Loss: 1.822556\n",
      "Epoch: 104, Batch: 199, Loss: 1.822345\n",
      "Epoch: 105, Batch: 199, Loss: 1.822285\n",
      "Epoch: 106, Batch: 199, Loss: 1.822213\n",
      "Epoch: 107, Batch: 199, Loss: 1.822189\n",
      "Epoch: 108, Batch: 199, Loss: 1.822151\n",
      "Epoch: 109, Batch: 199, Loss: 1.822156\n",
      "Epoch: 110, Batch: 199, Loss: 1.822208\n",
      "Epoch: 111, Batch: 199, Loss: 1.822279\n",
      "Epoch: 112, Batch: 199, Loss: 1.822224\n",
      "Epoch: 113, Batch: 199, Loss: 1.822176\n",
      "Epoch: 114, Batch: 199, Loss: 1.822195\n",
      "Epoch: 115, Batch: 199, Loss: 1.822344\n",
      "Epoch: 116, Batch: 199, Loss: 1.822101\n",
      "Epoch: 117, Batch: 199, Loss: 1.822167\n",
      "Epoch: 118, Batch: 199, Loss: 1.822489\n",
      "Epoch: 119, Batch: 199, Loss: 1.822200\n",
      "Epoch: 120, Batch: 199, Loss: 1.822113\n",
      "Epoch: 121, Batch: 199, Loss: 1.822065\n",
      "Epoch: 122, Batch: 199, Loss: 1.822108\n",
      "Epoch: 123, Batch: 199, Loss: 1.822041\n",
      "Epoch: 124, Batch: 199, Loss: 1.822029\n",
      "Epoch: 125, Batch: 199, Loss: 1.822116\n",
      "Epoch: 126, Batch: 199, Loss: 1.822025\n",
      "Epoch: 127, Batch: 199, Loss: 1.821984\n",
      "Epoch: 128, Batch: 199, Loss: 1.822207\n",
      "Epoch: 129, Batch: 199, Loss: 1.822474\n",
      "Epoch: 130, Batch: 199, Loss: 1.822242\n",
      "Epoch: 131, Batch: 199, Loss: 1.822174\n",
      "Epoch: 132, Batch: 199, Loss: 1.822064\n",
      "Epoch: 133, Batch: 199, Loss: 1.822001\n",
      "Epoch: 134, Batch: 199, Loss: 1.822029\n",
      "Epoch: 135, Batch: 199, Loss: 1.822197\n",
      "Epoch: 136, Batch: 199, Loss: 1.822261\n",
      "Epoch: 137, Batch: 199, Loss: 1.822182\n",
      "Epoch: 138, Batch: 199, Loss: 1.822063\n",
      "Epoch: 139, Batch: 199, Loss: 1.822023\n",
      "Epoch: 140, Batch: 199, Loss: 1.821980\n",
      "Epoch: 141, Batch: 199, Loss: 1.822058\n",
      "Epoch: 142, Batch: 199, Loss: 1.821977\n",
      "Epoch: 143, Batch: 199, Loss: 1.821958\n",
      "Epoch: 144, Batch: 199, Loss: 1.821959\n",
      "Epoch: 145, Batch: 199, Loss: 1.821954\n",
      "Epoch: 146, Batch: 199, Loss: 1.821911\n",
      "Epoch: 147, Batch: 199, Loss: 1.821950\n",
      "Epoch: 148, Batch: 199, Loss: 1.822446\n",
      "Epoch: 149, Batch: 199, Loss: 1.822216\n",
      "Epoch: 150, Batch: 199, Loss: 1.821954\n",
      "Epoch: 151, Batch: 199, Loss: 1.821890\n",
      "Epoch: 152, Batch: 199, Loss: 1.821901\n",
      "Epoch: 153, Batch: 199, Loss: 1.821944\n",
      "Epoch: 154, Batch: 199, Loss: 1.821930\n",
      "Epoch: 155, Batch: 199, Loss: 1.821907\n",
      "Epoch: 156, Batch: 199, Loss: 1.821883\n",
      "Epoch: 157, Batch: 199, Loss: 1.821900\n",
      "Epoch: 158, Batch: 199, Loss: 1.821878\n",
      "Epoch: 159, Batch: 199, Loss: 1.821925\n",
      "Epoch: 160, Batch: 199, Loss: 1.821894\n",
      "Epoch: 161, Batch: 199, Loss: 1.821896\n",
      "Epoch: 162, Batch: 199, Loss: 1.822177\n",
      "Epoch: 163, Batch: 199, Loss: 1.821958\n",
      "Epoch: 164, Batch: 199, Loss: 1.821989\n",
      "Epoch: 165, Batch: 199, Loss: 1.821900\n",
      "Epoch: 166, Batch: 199, Loss: 1.821861\n",
      "Epoch: 167, Batch: 199, Loss: 1.821843\n",
      "Epoch: 168, Batch: 199, Loss: 1.821841\n",
      "Epoch: 169, Batch: 199, Loss: 1.821863\n",
      "Epoch: 170, Batch: 199, Loss: 1.821879\n",
      "Epoch: 171, Batch: 199, Loss: 1.822419\n",
      "Epoch: 172, Batch: 199, Loss: 1.821891\n",
      "Epoch: 173, Batch: 199, Loss: 1.821871\n",
      "Epoch: 174, Batch: 199, Loss: 1.821881\n",
      "Epoch: 175, Batch: 199, Loss: 1.822424\n",
      "Epoch: 176, Batch: 199, Loss: 1.822369\n",
      "Epoch: 177, Batch: 199, Loss: 1.822039\n",
      "Epoch: 178, Batch: 199, Loss: 1.821987\n",
      "Epoch: 179, Batch: 199, Loss: 1.821888\n",
      "Epoch: 180, Batch: 199, Loss: 1.821824\n",
      "Epoch: 181, Batch: 199, Loss: 1.821817\n",
      "Epoch: 182, Batch: 199, Loss: 1.821866\n",
      "Epoch: 183, Batch: 199, Loss: 1.822143\n",
      "Epoch: 184, Batch: 199, Loss: 1.822038\n",
      "Epoch: 185, Batch: 199, Loss: 1.822012\n",
      "Epoch: 186, Batch: 199, Loss: 1.821925\n",
      "Epoch: 187, Batch: 199, Loss: 1.821966\n",
      "Epoch: 188, Batch: 199, Loss: 1.821964\n",
      "Epoch: 189, Batch: 199, Loss: 1.821955\n",
      "Epoch: 190, Batch: 199, Loss: 1.821888\n",
      "Epoch: 191, Batch: 199, Loss: 1.821828\n",
      "Epoch: 192, Batch: 199, Loss: 1.821837\n",
      "Epoch: 193, Batch: 199, Loss: 1.821826\n",
      "Epoch: 194, Batch: 199, Loss: 1.821792\n",
      "Epoch: 195, Batch: 199, Loss: 1.821802\n",
      "Epoch: 196, Batch: 199, Loss: 1.821770\n",
      "Epoch: 197, Batch: 199, Loss: 1.821798\n",
      "Epoch: 198, Batch: 199, Loss: 1.821814\n",
      "Epoch: 199, Batch: 199, Loss: 1.822641\n"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "dim_input = d_model\n",
    "dim_output = d_model\n",
    "dim_recurrent = d_model\n",
    "num_sequence = 27\n",
    "output = tgt_vocab_size\n",
    "src_padding_idx = Vocab_en.__getitem__('<Pad>')\n",
    "\n",
    "Model = GRUNet(dim_input, dim_recurrent, num_sequence, src_padding_idx, tgt_padding_idx, src_vocab_size,tgt_vocab_size)\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer LSTM\n",
    "optimizer = th.optim.Adam(Model.parameters(), lr=0.0001)\n",
    "\n",
    "#define the number of epochs\n",
    "n_epochs = 200\n",
    "batch_size = 50\n",
    "n_train = 10000\n",
    "\n",
    "# Define the number of batches\n",
    "n_batches = 200\n",
    "\n",
    "loss_batches = train(Model, optimizer, loss_fn, n_epochs, n_batches, X_train, Y_train)\n",
    "\n",
    "th.save(Model.state_dict(), \"Models/RNN.pt\")\n",
    "\n",
    "with open('Models/RnnLoss.pkl', 'wb') as f:\n",
    "    pl.dump(loss_batches,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,   6,   0,   0,   1,   5,   1,   1,   1,   4,   0, 205, 205,   3,\n",
       "        205, 205, 205, 205, 205, 205, 205, 205, 205, 205, 205, 205, 205])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model.to(\"cpu\")\n",
    "predict_token = th.tensor([Vocab_en.__getitem__('<Start>')],dtype = th.int32)\n",
    "\n",
    "predict_token_batch = th.zeros((n_batches,1),dtype=th.int32)\n",
    "predict_token_batch[:] = Vocab_fr.__getitem__('<Pad>')\n",
    "\n",
    "\n",
    "out = Model(X_train[0],predict_token)\n",
    "out[0].argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f2161e111f783a6322a6ae262a47844d9386d7dfb61a436620c434d93864cb0f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
