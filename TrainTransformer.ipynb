{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikolaj\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "\n",
    "import torch as th\n",
    "import torch as tn\n",
    "from torchtext import vocab\n",
    "import pickle as pl\n",
    "from Models.Transformer import Transformer\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "with open('data/English_encodings.pkl', 'rb') as f:\n",
    "    english_encodings,english_sentences,Paddings_en,Vocab_en = pl.load(f)\n",
    "with open('data/French_encodings.pkl', 'rb') as f:\n",
    "    french_encodings,french_sentences,Paddings_fr,Vocab_fr = pl.load(f)\n",
    "\n",
    "# Get the vocabulary size\n",
    "src_vocab_size = Vocab_fr.__len__()\n",
    "tgt_vocab_size = Vocab_en.__len__()\n",
    "src_padding_idx = Vocab_fr.__getitem__('<Pad>')\n",
    "tgt_padding_idx = Vocab_en.__getitem__('<Pad>')\n",
    "\n",
    "\n",
    "# Load Train,Vaildation and Test data\n",
    "with open('data/Train_data.pkl', 'rb') as f:\n",
    "    X_train,Y_train,src_padding_mask,tgt_padding_mask = pl.load(f)\n",
    "\n",
    "with open('data/Validation_data.pkl', 'rb') as f:\n",
    "    X_vali,Y_vali,src_padding_mask_vali = pl.load(f)\n",
    "\n",
    "with open('data/Test_data.pkl', 'rb') as f:\n",
    "    X_test,Y_test,src_padding_mask_test = pl.load(f)\n",
    "\n",
    "tgt_mask = th.triu(th.full((27, 27), float('-inf')), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predict_loss(Model,X,Y,d_model,device,tgt_mask,print_sentence = True):\n",
    "    loss_fn = tn.nn.CrossEntropyLoss()\n",
    "\n",
    "    Prediction = th.zeros(27,dtype=th.int32,device=device)\n",
    "    Prediction[:] = Vocab_en.__getitem__('<Pad>')\n",
    "    \n",
    "    Prediction_mask = th.zeros((27,d_model),device=device)\n",
    "\n",
    "    Prediction_vectors = th.zeros((27,tgt_vocab_size),device=device)\n",
    "    Prediction_vectors[0][204] = 1\n",
    "\n",
    "\n",
    "    Prediction[0] = Vocab_en.__getitem__('<Start>')\n",
    "    Prediction_mask[0] = 1\n",
    "\n",
    "    src_mask = tn.ones((27,d_model),device=device)\n",
    "    src_mask[-(X == Vocab_fr.__getitem__('<Pad>')).sum():] = 0\n",
    "\n",
    "\n",
    "    for i in range(1,27):\n",
    "        out = Model(X,Prediction,src_padding_mask = src_mask,tgt_mask=tgt_mask,tgt_padding_mask = Prediction_mask)\n",
    "        Prediction[i] = out.argmax(1)[i]\n",
    "        Prediction_vectors[i] = out[i]\n",
    "        Prediction_mask[i] = 1\n",
    "\n",
    "    \n",
    "    if print_sentence:\n",
    "        Senctence_pred = \"\"\n",
    "        for word in Vocab_en.lookup_tokens(Prediction.tolist()):\n",
    "            Senctence_pred += \" \" + word\n",
    "\n",
    "        print(\"Predicted Senctence:\")\n",
    "        print(Senctence_pred)\n",
    "        print(\"\")\n",
    "        Senctence_true = \"\"\n",
    "        for word in Vocab_en.lookup_tokens(Y.tolist()):\n",
    "            Senctence_true += \" \" + word\n",
    "        print(\"True Senctence:\")\n",
    "        print(Senctence_true)\n",
    "\n",
    "    loss = loss_fn(Prediction_vectors,Y)\n",
    "    #print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Function to train the model\n",
    "def train(model, src_data, tgt_data, src_padding_mask, tgt_padding_mask, tgt_mask, optimizer, loss_fn, epochs, X_vali,Y_vali):\n",
    "    \n",
    "    # Check if CUDA/mps is available\n",
    "    if tn.cuda.is_available():\n",
    "        device = tn.device(\"cuda\")\n",
    "    elif tn.backends.mps.is_available():\n",
    "        device = \"cpu\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    \n",
    "    # Move the model to the device\n",
    "    model.to(device)\n",
    "\n",
    "    # Move the data to the device\n",
    "    src_data = src_data.to(device)\n",
    "    tgt_data = tgt_data.to(device)\n",
    "    src_padding_mask = src_padding_mask.to(device)\n",
    "    tgt_padding_mask = tgt_padding_mask.to(device)\n",
    "    tgt_mask = tgt_mask.to(device)\n",
    "    X_vali = X_vali.to(device)\n",
    "    Y_vali = Y_vali.to(device)\n",
    "\n",
    "\n",
    "    \n",
    "    # Initialize the loss\n",
    "    loss_train = []\n",
    "    loss_vali = []\n",
    "    \n",
    "    n_batches,batch_size,_ = src_data.shape\n",
    "\n",
    "   \n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        # Initialize the loss\n",
    "        epoch_loss = 0\n",
    "        epoch_loss_vali = 0\n",
    "        # Train the model\n",
    "        \n",
    "        for src_batch,tgt_batch,src_pad,tgt_pad in zip(src_data,tgt_data,src_padding_mask,tgt_padding_mask):\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Batch loss\n",
    "            loss = 0\n",
    "\n",
    "            # Go trough each batch\n",
    "            for i in range(batch_size):\n",
    "                out = model(src_batch[i],tgt_batch[i],\n",
    "                        tgt_mask = tgt_mask,\n",
    "                        src_padding_mask = src_pad[i],\n",
    "                        tgt_padding_mask = tgt_pad[i]\n",
    "                        )\n",
    "\n",
    "                loss += loss_fn(out,tgt_batch[i])\n",
    "\n",
    "            # Backpropagate the loss\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Add the loss\n",
    "            epoch_loss += loss.item()\n",
    "            loss_train.append(loss.item())\n",
    "\n",
    "            loss_vali_val = 0\n",
    "            # Get validation loss\n",
    "            for sample in range(X_vali.shape[0]):\n",
    "                loss_vali_val += Predict_loss(model,X_vali[sample],Y_vali[sample],d_model=512,tgt_mask=tgt_mask,device=device,print_sentence=False)\n",
    "\n",
    "            loss_vali_val /= X_vali.shape[0]\n",
    "\n",
    "            loss_vali.append(loss_vali_val) \n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs} Loss: {epoch_loss/(n_batches*batch_size)}')\n",
    "\n",
    "       \n",
    "        \n",
    "    \n",
    "    return loss_train, loss_vali, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 8.00 GiB total capacity; 7.27 GiB already allocated; 0 bytes free; 7.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m optimizer \u001b[39m=\u001b[39m tn\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(Model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.0001\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m loss_train,vali_losses, Model \u001b[39m=\u001b[39m train(Model, X_train, Y_train, src_padding_mask, tgt_padding_mask, tgt_mask, optimizer, loss_fn, epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,X_vali\u001b[39m=\u001b[39;49mX_vali,Y_vali\u001b[39m=\u001b[39;49mY_vali)\n",
      "Cell \u001b[1;32mIn[3], line 117\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, src_data, tgt_data, src_padding_mask, tgt_padding_mask, tgt_mask, optimizer, loss_fn, epochs, X_vali, Y_vali)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39m# Get validation loss\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(X_vali\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[1;32m--> 117\u001b[0m     loss_vali_val \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m Predict_loss(model,X_vali[sample],Y_vali[sample],d_model\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m,tgt_mask\u001b[39m=\u001b[39;49mtgt_mask,device\u001b[39m=\u001b[39;49mdevice,print_sentence\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    119\u001b[0m loss_vali_val \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m X_vali\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m    121\u001b[0m loss_vali\u001b[39m.\u001b[39mappend(loss_vali_val) \n",
      "Cell \u001b[1;32mIn[3], line 21\u001b[0m, in \u001b[0;36mPredict_loss\u001b[1;34m(Model, X, Y, d_model, device, tgt_mask, print_sentence)\u001b[0m\n\u001b[0;32m     17\u001b[0m src_mask[\u001b[39m-\u001b[39m(X \u001b[39m==\u001b[39m Vocab_fr\u001b[39m.\u001b[39m\u001b[39m__getitem__\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m<Pad>\u001b[39m\u001b[39m'\u001b[39m))\u001b[39m.\u001b[39msum():] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m27\u001b[39m):\n\u001b[1;32m---> 21\u001b[0m     out \u001b[39m=\u001b[39m Model(X,Prediction,src_padding_mask \u001b[39m=\u001b[39;49m src_mask,tgt_mask\u001b[39m=\u001b[39;49mtgt_mask,tgt_padding_mask \u001b[39m=\u001b[39;49m Prediction_mask)\n\u001b[0;32m     22\u001b[0m     Prediction[i] \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39margmax(\u001b[39m1\u001b[39m)[i]\n\u001b[0;32m     23\u001b[0m     Prediction_vectors[i] \u001b[39m=\u001b[39m out[i]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Nikolaj\\Desktop\\Uni ting\\Deep learning architecture\\Transformer-Architecture\\Models\\Transformer.py:232\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, src, tgt, src_padding_mask, tgt_mask, tgt_padding_mask)\u001b[0m\n\u001b[0;32m    228\u001b[0m     tgt \u001b[39m=\u001b[39m tgt \u001b[39m*\u001b[39m tgt_padding_mask\n\u001b[0;32m    231\u001b[0m memory \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(X,)\n\u001b[1;32m--> 232\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(tgt, memory, tgt_mask)\n\u001b[0;32m    234\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLinear_out(output)\n\u001b[0;32m    235\u001b[0m \u001b[39m#out = self.Sotftmax(out)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Nikolaj\\Desktop\\Uni ting\\Deep learning architecture\\Transformer-Architecture\\Models\\Transformer.py:204\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[1;34m(self, memory, tgt, src_mask, tgt_mask, tgt_padding_mask)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, memory, tgt, src_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, tgt_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, tgt_padding_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m decoder \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoders:\n\u001b[1;32m--> 204\u001b[0m         tgt \u001b[39m=\u001b[39m decoder(memory,tgt,src_mask)\n\u001b[0;32m    205\u001b[0m     Y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(tgt)\n\u001b[0;32m    206\u001b[0m     \u001b[39mreturn\u001b[39;00m Y\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Nikolaj\\Desktop\\Uni ting\\Deep learning architecture\\Transformer-Architecture\\Models\\Transformer.py:184\u001b[0m, in \u001b[0;36mDecoderLayer.forward\u001b[1;34m(self, memory, tgt, mask)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,memory,tgt,mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    183\u001b[0m     mha1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mMHA1(tgt,tgt,tgt,mask)\n\u001b[1;32m--> 184\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm1(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout1(mha1) \u001b[39m+\u001b[39m tgt)\n\u001b[0;32m    186\u001b[0m     mha2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mMHA2(memory,memory,x,mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    188\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout2(mha2) \u001b[39m+\u001b[39m x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\functional.py:1252\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1250\u001b[0m \u001b[39mif\u001b[39;00m p \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mor\u001b[39;00m p \u001b[39m>\u001b[39m \u001b[39m1.0\u001b[39m:\n\u001b[0;32m   1251\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdropout probability has to be between 0 and 1, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(p))\n\u001b[1;32m-> 1252\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mdropout_(\u001b[39minput\u001b[39m, p, training) \u001b[39mif\u001b[39;00m inplace \u001b[39melse\u001b[39;00m _VF\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, p, training)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 8.00 GiB total capacity; 7.27 GiB already allocated; 0 bytes free; 7.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Intialize the model with set hyperparameters\n",
    "T = 27\n",
    "d_model = 512 # Dimension of the model (Embedding size)\n",
    "d_ff = 2048 # Dimension of the feedforward network model in transformer\n",
    "nhead = 8 # Number of heads in the multiheadattention models\n",
    "dk = d_model//nhead\n",
    "dv = d_model//nhead\n",
    "num_layers = 6\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "Model = Transformer(\n",
    "    T = T,\n",
    "    d_model = d_model,\n",
    "    nhead = nhead,\n",
    "    d_ff = d_ff,\n",
    "    dk = dk,\n",
    "    dv = dv,\n",
    "    num_layers = num_layers,\n",
    "    src_vocab_size = src_vocab_size,\n",
    "    tgt_vocab_size = tgt_vocab_size,\n",
    "    src_padding_idx = src_padding_idx,\n",
    "    tgt_padding_idx = tgt_padding_idx,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "loss_fn = tn.nn.CrossEntropyLoss()\n",
    "optimizer = tn.optim.Adam(Model.parameters(), lr=0.0001)\n",
    "\n",
    "# Train the model\n",
    "loss_train,vali_losses, Model = train(Model, X_train, Y_train, src_padding_mask, tgt_padding_mask, tgt_mask, optimizer, loss_fn, epochs=1,X_vali=X_vali,Y_vali=Y_vali)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn.save(Model.state_dict(), \"Models/Transformer.pt\")\n",
    "\n",
    "\n",
    "\n",
    "with open('Models/TransformerLoss.pkl', 'wb') as f:\n",
    "    pl.dump([loss_train, vali_losses] ,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predict(X,print_sentence = True):\n",
    "    Prediction = th.zeros(27,dtype=th.int32)\n",
    "    Prediction[:] = Vocab_en.__getitem__('<Pad>')\n",
    "    Prediction_mask = th.zeros((27,128))\n",
    "\n",
    "    Prediction[0] = Vocab_en.__getitem__('<Start>')\n",
    "    Prediction_mask[0] = 1\n",
    "\n",
    "    src_mask = tn.ones((27,128))\n",
    "    src_mask[-(X == Vocab_fr.__getitem__('<Pad>')).sum():] = 0\n",
    "\n",
    "    for i in range(1,27):\n",
    "        out = Model(X_test[0],Prediction,src_padding_mask = src_mask,tgt_mask=tgt_mask,tgt_padding_mask = Prediction_mask)\n",
    "        Prediction[i] = out.argmax(1)[i]\n",
    "        Prediction_mask[i] = 1\n",
    "\n",
    "    \n",
    "    if print_sentence:\n",
    "        Senctence = \"\"\n",
    "        for word in Vocab_en.lookup_tokens(Prediction.tolist()):\n",
    "            Senctence += \" \" + word\n",
    "\n",
    "        print(Senctence)\n",
    "\n",
    "    \n",
    "    return Prediction,src_mask\n",
    "\n",
    "def Predict_loss(X,Y,print_sentence = True):\n",
    "\n",
    "    Prediction = th.zeros(27,dtype=th.int32)\n",
    "    Prediction[:] = Vocab_en.__getitem__('<Pad>')\n",
    "    \n",
    "    Prediction_mask = th.zeros((27,d_model))\n",
    "\n",
    "    Prediction_vectors = th.zeros((27,tgt_vocab_size))\n",
    "    Prediction_vectors[0][204] = 1\n",
    "\n",
    "\n",
    "    Prediction[0] = Vocab_en.__getitem__('<Start>')\n",
    "    Prediction_mask[0] = 1\n",
    "\n",
    "    src_mask = tn.ones((27,d_model))\n",
    "    src_mask[-(X == Vocab_fr.__getitem__('<Pad>')).sum():] = 0\n",
    "\n",
    "    for i in range(1,27):\n",
    "        out = Model(X,Prediction,src_padding_mask = src_mask,tgt_mask=tgt_mask,tgt_padding_mask = Prediction_mask)\n",
    "        Prediction[i] = out.argmax(1)[i]\n",
    "        Prediction_vectors[i] = out[i]\n",
    "        Prediction_mask[i] = 1\n",
    "\n",
    "    \n",
    "    if print_sentence:\n",
    "        Senctence_pred = \"\"\n",
    "        for word in Vocab_en.lookup_tokens(Prediction.tolist()):\n",
    "            Senctence_pred += \" \" + word\n",
    "\n",
    "        print(\"Predicted Senctence:\")\n",
    "        print(Senctence_pred)\n",
    "        print(\"\")\n",
    "        Senctence_true = \"\"\n",
    "        for word in Vocab_en.lookup_tokens(Y.tolist()):\n",
    "            Senctence_true += \" \" + word\n",
    "        print(\"True Senctence:\")\n",
    "        print(Senctence_true)\n",
    "\n",
    "    loss = loss_fn(Prediction_vectors,Y)\n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Senctence:\n",
      " <Start> her least favorite fruit is the banana , but your least favorite is the lime . <End> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad>\n",
      "\n",
      "True Senctence:\n",
      " <Start> her least favorite fruit is the banana , but your least favorite is the lime . <End> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad>\n",
      "Loss: 0.2475370168685913\n",
      "Predicted Senctence:\n",
      " <Start> the united states is warm may may may and it is is quiet in august . <End> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad>\n",
      "\n",
      "True Senctence:\n",
      " <Start> the united states is nice during may , and it is quiet in august . <End> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad>\n",
      "Loss: 2.326324224472046\n",
      "Predicted Senctence:\n",
      " <Start> california is sometimes warm during june , but it is busy snowy autumn autumn . <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad>\n",
      "\n",
      "True Senctence:\n",
      " <Start> california is sometimes hot during june , but it is usually snowy in fall . <End> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad>\n",
      "Loss: 0.7047181725502014\n",
      "Predicted Senctence:\n",
      " <Start> the peach is their favorite fruit fruit , but the apple is our least favorite . <End> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad>\n",
      "\n",
      "True Senctence:\n",
      " <Start> the peach is their least favorite fruit , but the apple is our least favorite . <End> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad>\n",
      "Loss: 0.3412400782108307\n",
      "Predicted Senctence:\n",
      " <Start> california is usually dry during may but november is is busy busy autumn <End> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad>\n",
      "\n",
      "True Senctence:\n",
      " <Start> california is usually dry during november , but it is usually chilly in may . <End> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad>\n",
      "Loss: 1.7566590309143066\n"
     ]
    }
   ],
   "source": [
    "Model.to(\"cpu\")\n",
    "for i in range(5):\n",
    "    Predict_loss(X_test[i],Y_test[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16 (main, Dec  7 2022, 10:02:13) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "554043c00383563873f7de553f5ef220995ac86d49372a9e11c31629c859d6a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
