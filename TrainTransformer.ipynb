{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/magnusharder/Documents/UNI-DTU/6. Semester/Arkitektur - Special Course/.venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "\n",
    "import torch as th\n",
    "import torch as tn\n",
    "from torchtext import vocab\n",
    "import pickle as pl\n",
    "from Models.Transformer import Transformer\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "with open('data/English_encodings.pkl', 'rb') as f:\n",
    "    english_encodings,english_sentences,Paddings_en,Vocab_en = pl.load(f)\n",
    "with open('data/French_encodings.pkl', 'rb') as f:\n",
    "    french_encodings,french_sentences,Paddings_fr,Vocab_fr = pl.load(f)\n",
    "\n",
    "# Get the vocabulary size\n",
    "src_vocab_size = Vocab_fr.__len__()\n",
    "tgt_vocab_size = Vocab_en.__len__()\n",
    "src_padding_idx = Vocab_fr.__getitem__('<Pad>')\n",
    "tgt_padding_idx = Vocab_en.__getitem__('<Pad>')\n",
    "\n",
    "\n",
    "# Load Train,Vaildation and Test data\n",
    "with open('data/Train_data.pkl', 'rb') as f:\n",
    "    X_train_batches,Y_train_batches,src_padding_mask,tgt_padding_mask = pl.load(f)\n",
    "\n",
    "with open('data/Validation_data.pkl', 'rb') as f:\n",
    "    X_vali_batches,Y_vali_batches,src_padding_mask_vali = pl.load(f)\n",
    "\n",
    "with open('data/Test_data.pkl', 'rb') as f:\n",
    "    X_test_batches,Y_test_batches,src_padding_mask_test = pl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model\n",
    "def train(model, src_data, tgt_data, src_padding_mask, tgt_padding_mask, optimizer, loss_fn, epochs, Model_Params=None):\n",
    "    \n",
    "    # Check if CUDA/mps is available\n",
    "    if tn.cuda.is_available():\n",
    "        device = tn.device(\"cuda\")\n",
    "    elif tn.backends.mps.is_available():\n",
    "        device = \"cpu\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    \n",
    "    # Move the model to the device\n",
    "    model.to(device)\n",
    "\n",
    "    # Move the data to the device\n",
    "    src_data = src_data.to(device)\n",
    "    tgt_data = tgt_data.to(device)\n",
    "    src_padding_mask = src_padding_mask.to(device)\n",
    "    tgt_padding_mask = tgt_padding_mask.to(device)\n",
    "    \n",
    "    \n",
    "    # Initialize the loss\n",
    "    loss_train = []\n",
    "    loss_vali = []\n",
    "    \n",
    "    n_batches,batch_size,_ = src_data.shape\n",
    "\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        # Initialize the loss\n",
    "        epoch_loss = 0\n",
    "        epoch_loss_vali = 0\n",
    "        # Train the model\n",
    "        for src_batch,tgt_batch,src_pad,tgt_pad in zip(src_data,tgt_data,src_padding_mask,tgt_padding_mask):\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Batch loss\n",
    "            loss = 0\n",
    "\n",
    "            # Go trough each batch\n",
    "            for i in range(batch_size):\n",
    "                out = model(src_batch[i],tgt_batch[i],\n",
    "                        src_padding_mask = src_pad[i],\n",
    "                        tgt_padding_mask = tgt_pad[i]\n",
    "                        )\n",
    "\n",
    "                loss += loss_fn(out,tgt_batch[i])\n",
    "            \n",
    "            # Backpropagate the loss\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Add the loss\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            \n",
    "        \n",
    "        # Validation loss\n",
    "\n",
    "        loss_train.append(epoch_loss/(n_batches*batch_size))\n",
    "    \n",
    "    return loss_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialize the model with set hyperparameters\n",
    "T = 27\n",
    "d_model = 128 # Dimension of the model (Embedding size)\n",
    "d_ff = 256 # Dimension of the feedforward network model in transformer\n",
    "nhead = 3 # Number of heads in the multiheadattention models\n",
    "dk = 64\n",
    "dv = 64\n",
    "num_layers = 3\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "Model = Transformer(\n",
    "    T = T,\n",
    "    d_model = d_model,\n",
    "    nhead = nhead,\n",
    "    d_ff = d_ff,\n",
    "    dk = 64,\n",
    "    dv = 64,\n",
    "    num_layers = num_layers,\n",
    "    src_vocab_size = src_vocab_size,\n",
    "    tgt_vocab_size = tgt_vocab_size,\n",
    "    src_padding_idx = src_padding_idx,\n",
    "    tgt_padding_idx = tgt_padding_idx,\n",
    "    dropout=0.1\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'loss_epoch' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m optimizer \u001b[39m=\u001b[39m tn\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(Model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m loss_train \u001b[39m=\u001b[39m train(Model, X_train_batches, Y_train_batches, src_padding_mask, tgt_padding_mask, optimizer, loss_fn, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[13], line 59\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, src_data, tgt_data, src_padding_mask, tgt_padding_mask, optimizer, loss_fn, epochs, Model_Params)\u001b[0m\n\u001b[1;32m     56\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     58\u001b[0m     \u001b[39m# Add the loss\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m     loss_epoch \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     63\u001b[0m \u001b[39m# Validation loss\u001b[39;00m\n\u001b[1;32m     65\u001b[0m loss_train\u001b[39m.\u001b[39mappend(loss_epoch\u001b[39m/\u001b[39m(n_batches\u001b[39m*\u001b[39mbatch_size))\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'loss_epoch' referenced before assignment"
     ]
    }
   ],
   "source": [
    "loss_fn = tn.nn.CrossEntropyLoss()\n",
    "optimizer = tn.optim.Adam(Model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "loss_train = train(Model, X_train_batches, Y_train_batches, src_padding_mask, tgt_padding_mask, optimizer, loss_fn, epochs=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 50, 27])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_batches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 0.6889562835693359\n",
      "Epoch: 1 Loss: 0.6403594646453857\n",
      "Epoch: 2 Loss: 0.6005053577423096\n",
      "Epoch: 3 Loss: 0.5656527500152588\n",
      "Epoch: 4 Loss: 0.5337777290344238\n",
      "Epoch: 5 Loss: 0.5060778198242187\n",
      "Epoch: 6 Loss: 0.48012523460388185\n",
      "Epoch: 7 Loss: 0.45571697425842284\n",
      "Epoch: 8 Loss: 0.43140914344787595\n",
      "Epoch: 9 Loss: 0.40972098922729494\n",
      "Epoch: 10 Loss: 0.3919607830047607\n",
      "Epoch: 11 Loss: 0.37960798835754395\n",
      "Epoch: 12 Loss: 0.3641638145446777\n",
      "Epoch: 13 Loss: 0.3367669448852539\n",
      "Epoch: 14 Loss: 0.3162376413345337\n",
      "Epoch: 15 Loss: 0.3018227119445801\n",
      "Epoch: 16 Loss: 0.2860628709793091\n",
      "Epoch: 17 Loss: 0.26723638820648193\n",
      "Epoch: 18 Loss: 0.2516777753829956\n",
      "Epoch: 19 Loss: 0.24089940643310548\n",
      "Epoch: 20 Loss: 0.2339546661376953\n",
      "Epoch: 21 Loss: 0.2102954912185669\n",
      "Epoch: 22 Loss: 0.18968856048583985\n",
      "Epoch: 23 Loss: 0.1738504629135132\n",
      "Epoch: 24 Loss: 0.16169569826126098\n",
      "Epoch: 25 Loss: 0.15234423685073853\n",
      "Epoch: 26 Loss: 0.1456462516784668\n",
      "Epoch: 27 Loss: 0.13956336879730225\n",
      "Epoch: 28 Loss: 0.13392937755584716\n",
      "Epoch: 29 Loss: 0.1237392897605896\n",
      "Epoch: 30 Loss: 0.11310457944869995\n",
      "Epoch: 31 Loss: 0.10082652091979981\n",
      "Epoch: 32 Loss: 0.09294352436065674\n",
      "Epoch: 33 Loss: 0.08915649366378785\n",
      "Epoch: 34 Loss: 0.0862819139957428\n",
      "Epoch: 35 Loss: 0.08261072754859924\n",
      "Epoch: 36 Loss: 0.07554757237434387\n",
      "Epoch: 37 Loss: 0.07035695219039917\n",
      "Epoch: 38 Loss: 0.0663492670059204\n",
      "Epoch: 39 Loss: 0.06152002930641174\n",
      "Epoch: 40 Loss: 0.056671246528625485\n",
      "Epoch: 41 Loss: 0.05279784035682678\n",
      "Epoch: 42 Loss: 0.049094581365585324\n",
      "Epoch: 43 Loss: 0.04498326694965363\n",
      "Epoch: 44 Loss: 0.0416609765291214\n",
      "Epoch: 45 Loss: 0.038894465088844296\n",
      "Epoch: 46 Loss: 0.03650214469432831\n",
      "Epoch: 47 Loss: 0.03442700779438019\n",
      "Epoch: 48 Loss: 0.03251368355751037\n",
      "Epoch: 49 Loss: 0.03099699890613556\n",
      "Epoch: 50 Loss: 0.029759887337684632\n",
      "Epoch: 51 Loss: 0.028458539724349977\n",
      "Epoch: 52 Loss: 0.027123692631721496\n",
      "Epoch: 53 Loss: 0.025748544096946716\n",
      "Epoch: 54 Loss: 0.024436021864414216\n",
      "Epoch: 55 Loss: 0.023217319667339326\n",
      "Epoch: 56 Loss: 0.022255714356899263\n",
      "Epoch: 57 Loss: 0.02139479112625122\n",
      "Epoch: 58 Loss: 0.020579889118671417\n",
      "Epoch: 59 Loss: 0.019737869441509247\n",
      "Epoch: 60 Loss: 0.01893626403808594\n",
      "Epoch: 61 Loss: 0.01809089434146881\n",
      "Epoch: 62 Loss: 0.01744921875\n",
      "Epoch: 63 Loss: 0.01675890326499939\n",
      "Epoch: 64 Loss: 0.0161272936463356\n",
      "Epoch: 65 Loss: 0.015591374456882476\n",
      "Epoch: 66 Loss: 0.015044480085372925\n",
      "Epoch: 67 Loss: 0.01470978045463562\n",
      "Epoch: 68 Loss: 0.014200869023799896\n",
      "Epoch: 69 Loss: 0.013888819575309753\n",
      "Epoch: 70 Loss: 0.013522155702114106\n",
      "Epoch: 71 Loss: 0.013177507162094116\n",
      "Epoch: 72 Loss: 0.012845784842967988\n",
      "Epoch: 73 Loss: 0.012422656685113907\n",
      "Epoch: 74 Loss: 0.012105116248130798\n",
      "Epoch: 75 Loss: 0.011783255189657211\n",
      "Epoch: 76 Loss: 0.011525902837514878\n",
      "Epoch: 77 Loss: 0.011249858021736146\n",
      "Epoch: 78 Loss: 0.011055793434381485\n",
      "Epoch: 79 Loss: 0.010758454203605652\n",
      "Epoch: 80 Loss: 0.0105622898042202\n",
      "Epoch: 81 Loss: 0.010284685760736465\n",
      "Epoch: 82 Loss: 0.010106143206357956\n",
      "Epoch: 83 Loss: 0.009902574092149735\n",
      "Epoch: 84 Loss: 0.009706086844205857\n",
      "Epoch: 85 Loss: 0.009415698260068894\n",
      "Epoch: 86 Loss: 0.009198897540569306\n",
      "Epoch: 87 Loss: 0.008830917328596114\n",
      "Epoch: 88 Loss: 0.008685618191957473\n",
      "Epoch: 89 Loss: 0.008336702406406403\n",
      "Epoch: 90 Loss: 0.008306694656610488\n",
      "Epoch: 91 Loss: 0.008010297864675522\n",
      "Epoch: 92 Loss: 0.008031931668519975\n",
      "Epoch: 93 Loss: 0.007877749353647233\n",
      "Epoch: 94 Loss: 0.0077766130268573765\n",
      "Epoch: 95 Loss: 0.007572055488824845\n",
      "Epoch: 96 Loss: 0.007291400760412216\n",
      "Epoch: 97 Loss: 0.006990589022636414\n",
      "Epoch: 98 Loss: 0.006658638268709183\n",
      "Epoch: 99 Loss: 0.00642739924788475\n"
     ]
    }
   ],
   "source": [
    "trainmodel(100)\n",
    "\n",
    "tn.save(Model.state_dict(), \"Transformer.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([206,  18,  24,   0,   9,  68,   5,  40,   1,   8,   4,   0,  56,   3,\n",
       "         45,   2, 204, 205, 205, 205, 205, 205, 205, 205, 205, 205, 205])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_mask = th.full((T,T),float('-inf')).triu(diagonal=1)\n",
    "out = Model(X_train[0],Y_train[0],src_padding_mask = src_key_masks[0][0],tgt_padding_mask = tgt_key_masks[0][0],tgt_mask = tgt_mask)\n",
    "out.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([206,  18,  24,   0,   9,  68,   5,  40,   1,   8,   4,   0,  56,   3,\n",
       "         45,   2, 204, 205, 205, 205, 205, 205, 205, 205, 205, 205, 205])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prediction = th.zeros(27,dtype=th.int32)\n",
    "Prediction[:] = Vocab_en.__getitem__('<Pad>')\n",
    "Prediction_mask = th.zeros((27,128))\n",
    "\n",
    "Prediction[0] = Vocab_en.__getitem__('<Start>')\n",
    "Prediction_mask[0] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0]\n",
    "src_mask = tn.ones((27,128))\n",
    "src_mask[-Paddings_fr[0]:] = 0\n",
    "src_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([206,  32,  12,  12,  14,   0,   6,  14,   1,   7,   0,  13,  12,   0,\n",
       "          6,  84,   2, 204, 205, 205, 205, 205, 205, 205, 205, 205, 205],\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Prediction = th.zeros(27,dtype=th.int32)\n",
    "Prediction[:] = Vocab_en.__getitem__('<Pad>')\n",
    "Prediction_mask = th.zeros((27,128))\n",
    "\n",
    "Prediction[0] = Vocab_en.__getitem__('<Start>')\n",
    "Prediction_mask[0] = 1\n",
    "\n",
    "for i in range(1,27):\n",
    "    out = Model(X_test[0],Prediction,tgt_mask = tgt_mask,src_padding_mask = src_mask,tgt_padding_mask = Prediction_mask)\n",
    "    Prediction[i] = out.argmax(1)[i]\n",
    "    Prediction_mask[i] = 1\n",
    "\n",
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predict(X,print_sentence = True):\n",
    "    Prediction = th.zeros(27,dtype=th.int32)\n",
    "    Prediction[:] = Vocab_en.__getitem__('<Pad>')\n",
    "    Prediction_mask = th.zeros((27,128))\n",
    "\n",
    "    Prediction[0] = Vocab_en.__getitem__('<Start>')\n",
    "    Prediction_mask[0] = 1\n",
    "\n",
    "    for i in range(1,27):\n",
    "        out = Model(X_test[0],Prediction,tgt_mask = tgt_mask,src_padding_mask = src_mask,tgt_padding_mask = Prediction_mask)\n",
    "        Prediction[i] = out.argmax(1)[i]\n",
    "        Prediction_mask[i] = 1\n",
    "\n",
    "    \n",
    "    if print_sentence:\n",
    "        Senctence = \"\"\n",
    "        for word in Vocab_en.lookup_tokens(Prediction.tolist()):\n",
    "            Senctence += \" \" + word\n",
    "\n",
    "        print(Senctence)\n",
    "\n",
    "    \n",
    "    return Prediction\n",
    "\n",
    "def Predict_loss(X,Y,print_sentence = True):\n",
    "    Prediction = th.zeros(27,dtype=th.int32)\n",
    "    Prediction[:] = Vocab_en.__getitem__('<Pad>')\n",
    "    Prediction_mask = th.zeros((27,128))\n",
    "\n",
    "    Prediction_vectors = th.zeros((27,207))\n",
    "    Prediction_vectors[0][204] = 1\n",
    "\n",
    "\n",
    "    Prediction[0] = Vocab_en.__getitem__('<Start>')\n",
    "    Prediction_mask[0] = 1\n",
    "\n",
    "    src_mask = tn.ones((27,128))\n",
    "    src_mask[-(X == Vocab_fr.__getitem__('<Pad>')).sum():] = 0\n",
    "\n",
    "    for i in range(1,27):\n",
    "        out = Model(X,Prediction,tgt_mask = tgt_mask,src_padding_mask = src_mask,tgt_padding_mask = Prediction_mask)\n",
    "        Prediction[i] = out.argmax(1)[i]\n",
    "        Prediction_vectors[i] = out[i]\n",
    "        Prediction_mask[i] = 1\n",
    "\n",
    "    \n",
    "    if print_sentence:\n",
    "        Senctence_pred = \"\"\n",
    "        for word in Vocab_en.lookup_tokens(Prediction.tolist()):\n",
    "            Senctence_pred += \" \" + word\n",
    "\n",
    "        print(\"Predicted Senctence:\")\n",
    "        print(Senctence_pred)\n",
    "        print(\"\")\n",
    "        Senctence_true = \"\"\n",
    "        for word in Vocab_en.lookup_tokens(Y.tolist()):\n",
    "            Senctence_true += \" \" + word\n",
    "        print(\"True Senctence:\")\n",
    "        print(Senctence_true)\n",
    "\n",
    "    loss = loss_fn(Prediction_vectors,Y)\n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Senctence:\n",
      " <Start> california is sometimes warm during fall , and it is usually busy in july . <End> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad>\n",
      "\n",
      "True Senctence:\n",
      " <Start> california is sometimes hot during june , but it is usually snowy in fall . <End> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad>\n"
     ]
    }
   ],
   "source": [
    "l = Predict_loss(X_test[2],Y_test[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "554043c00383563873f7de553f5ef220995ac86d49372a9e11c31629c859d6a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
