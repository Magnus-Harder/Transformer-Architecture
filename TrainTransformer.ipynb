{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikolaj\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "import torch as tn\n",
    "import torch as th\n",
    "from torchtext import vocab\n",
    "import pickle as pl\n",
    "from Models.Transformer import Transformer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "with open('data/English_encodings.pkl', 'rb') as f:\n",
    "    english_encodings,english_sentences,Paddings_en,Vocab_en = pl.load(f)\n",
    "with open('data/French_encodings.pkl', 'rb') as f:\n",
    "    french_encodings,french_sentences,Paddings_fr,Vocab_fr = pl.load(f)\n",
    "\n",
    "# Get the vocabulary size\n",
    "src_vocab_size = Vocab_fr.__len__()\n",
    "tgt_vocab_size = Vocab_en.__len__()\n",
    "src_padding_idx = Vocab_fr.__getitem__('<Pad>')\n",
    "tgt_padding_idx = Vocab_en.__getitem__('<Pad>')\n",
    "\n",
    "\n",
    "# Load Train,Vaildation and Test data\n",
    "with open('data/Train_data.pkl', 'rb') as f:\n",
    "    X_train,Y_train,src_padding_mask,tgt_padding_mask = pl.load(f)\n",
    "\n",
    "with open('data/Validation_data.pkl', 'rb') as f:\n",
    "    X_vali,Y_vali,src_padding_mask_vali = pl.load(f)\n",
    "\n",
    "with open('data/Test_data.pkl', 'rb') as f:\n",
    "    X_test,Y_test,src_padding_mask_test = pl.load(f)\n",
    "\n",
    "tgt_mask = th.triu(th.full((27, 27), float('-inf')), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to train the model\n",
    "def train(model, src_data, tgt_data, src_padding_mask, tgt_padding_mask, tgt_mask, optimizer, loss_fn, epochs):\n",
    "    \n",
    "    # Check if CUDA/mps is available\n",
    "    if tn.cuda.is_available():\n",
    "        device = tn.device(\"cuda\")\n",
    "    elif tn.backends.mps.is_available():\n",
    "        device = \"cpu\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    \n",
    "    # Move the model to the device\n",
    "    model.to(device)\n",
    "\n",
    "    # Move the data to the device\n",
    "    src_data = src_data.to(device)\n",
    "    tgt_data = tgt_data.to(device)\n",
    "    src_padding_mask = src_padding_mask.to(device)\n",
    "    tgt_padding_mask = tgt_padding_mask.to(device)\n",
    "    tgt_mask = tgt_mask.to(device)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # Initialize the loss\n",
    "    loss_train = []\n",
    "    \n",
    "    n_batches,batch_size,_ = src_data.shape\n",
    "\n",
    "   \n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        # Initialize the loss\n",
    "        epoch_loss = 0\n",
    "        # Train the model\n",
    "        \n",
    "        for src_batch,tgt_batch,src_pad,tgt_pad in zip(src_data,tgt_data,src_padding_mask,tgt_padding_mask):\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Batch loss\n",
    "            loss = 0\n",
    "\n",
    "            # Go trough each batch\n",
    "            for i in range(batch_size):\n",
    "                out = model(src_batch[i],tgt_batch[i],\n",
    "                        tgt_mask = tgt_mask,\n",
    "                        src_padding_mask = src_pad[i],\n",
    "                        tgt_padding_mask = tgt_pad[i]\n",
    "                        )\n",
    "\n",
    "                loss += loss_fn(out,tgt_batch[i])\n",
    "\n",
    "            # Backpropagate the loss\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Add the loss\n",
    "            epoch_loss += loss.item()\n",
    "            loss_train.append(loss.item())\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs} Loss: {epoch_loss/(n_batches*batch_size)}')\n",
    "\n",
    "    \n",
    "    return loss_train, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Loss: 0.7038267760276794\n",
      "Epoch 2/10 Loss: 0.09483484647274017\n",
      "Epoch 3/10 Loss: 0.028080702769756317\n",
      "Epoch 4/10 Loss: 0.012959643244743347\n",
      "Epoch 5/10 Loss: 0.007870923562347889\n",
      "Epoch 6/10 Loss: 0.005108638553321361\n",
      "Epoch 7/10 Loss: 0.0037165282789617775\n",
      "Epoch 8/10 Loss: 0.0027161712914705277\n",
      "Epoch 9/10 Loss: 0.0019443184860050678\n",
      "Epoch 10/10 Loss: 0.0014907017659395933\n"
     ]
    }
   ],
   "source": [
    "# Intialize the model with set hyperparameters\n",
    "T = 27\n",
    "d_model = 512 # Dimension of the model (Embedding size)\n",
    "d_ff = 2048 # Dimension of the feedforward network model in transformer\n",
    "nhead = 8 # Number of heads in the multiheadattention models\n",
    "dk = d_model//nhead\n",
    "dv = d_model//nhead\n",
    "num_layers = 6\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "Model = Transformer(\n",
    "    T = T,\n",
    "    d_model = d_model,\n",
    "    nhead = nhead,\n",
    "    d_ff = d_ff,\n",
    "    dk = dk,\n",
    "    dv = dv,\n",
    "    num_layers = num_layers,\n",
    "    src_vocab_size = src_vocab_size,\n",
    "    tgt_vocab_size = tgt_vocab_size,\n",
    "    src_padding_idx = src_padding_idx,\n",
    "    tgt_padding_idx = tgt_padding_idx,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "loss_fn = tn.nn.CrossEntropyLoss()\n",
    "optimizer = tn.optim.Adam(Model.parameters(), lr=0.0001)\n",
    "\n",
    "# Train the model\n",
    "loss_train, Model = train(Model, X_train, Y_train, src_padding_mask, tgt_padding_mask, tgt_mask, optimizer, loss_fn, epochs=10)\n",
    "\n",
    "\n",
    "tn.save(Model.state_dict(), \"Models/Transformer.pt\")\n",
    "\n",
    "with open('Models/TransformerLoss.pkl', 'wb') as f:\n",
    "    pl.dump([loss_train] ,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predict(X,print_sentence = True):\n",
    "    Prediction = th.zeros(27,dtype=th.int32)\n",
    "    Prediction[:] = Vocab_en.__getitem__('<Pad>')\n",
    "    Prediction_mask = th.zeros((27,128))\n",
    "\n",
    "    Prediction[0] = Vocab_en.__getitem__('<Start>')\n",
    "    Prediction_mask[0] = 1\n",
    "\n",
    "    src_mask = tn.ones((27,128))\n",
    "    src_mask[-(X == Vocab_fr.__getitem__('<Pad>')).sum():] = 0\n",
    "\n",
    "    for i in range(1,27):\n",
    "        out = Model(X_test[0],Prediction,src_padding_mask = src_mask,tgt_mask=tgt_mask,tgt_padding_mask = Prediction_mask)\n",
    "        Prediction[i] = out.argmax(1)[i]\n",
    "        Prediction_mask[i] = 1\n",
    "\n",
    "    \n",
    "    if print_sentence:\n",
    "        Senctence = \"\"\n",
    "        for word in Vocab_en.lookup_tokens(Prediction.tolist()):\n",
    "            Senctence += \" \" + word\n",
    "\n",
    "        print(Senctence)\n",
    "\n",
    "    \n",
    "    return Prediction,src_mask\n",
    "\n",
    "def Predict_loss(X,Y,print_sentence = True):\n",
    "\n",
    "    Prediction = th.zeros(27,dtype=th.int32)\n",
    "    Prediction[:] = Vocab_en.__getitem__('<Pad>')\n",
    "    \n",
    "    Prediction_mask = th.zeros((27,d_model))\n",
    "\n",
    "    Prediction_vectors = th.zeros((27,tgt_vocab_size))\n",
    "    Prediction_vectors[0][204] = 1\n",
    "\n",
    "\n",
    "    Prediction[0] = Vocab_en.__getitem__('<Start>')\n",
    "    Prediction_mask[0] = 1\n",
    "\n",
    "    src_mask = tn.ones((27,d_model))\n",
    "    src_mask[-(X == Vocab_fr.__getitem__('<Pad>')).sum():] = 0\n",
    "\n",
    "    for i in range(1,27):\n",
    "        out = Model(X,Prediction,src_padding_mask = src_mask,tgt_mask=tgt_mask,tgt_padding_mask = Prediction_mask)\n",
    "        Prediction[i] = out.argmax(1)[i]\n",
    "        Prediction_vectors[i] = out[i]\n",
    "        Prediction_mask[i] = 1\n",
    "\n",
    "    \n",
    "    if print_sentence:\n",
    "        Senctence_pred = \"\"\n",
    "        for word in Vocab_en.lookup_tokens(Prediction.tolist()):\n",
    "            Senctence_pred += \" \" + word\n",
    "\n",
    "        print(\"Predicted Senctence:\")\n",
    "        print(Senctence_pred)\n",
    "        print(\"\")\n",
    "        Senctence_true = \"\"\n",
    "        for word in Vocab_en.lookup_tokens(Y.tolist()):\n",
    "            Senctence_true += \" \" + word\n",
    "        print(\"True Senctence:\")\n",
    "        print(Senctence_true)\n",
    "\n",
    "    loss = loss_fn(Prediction_vectors,Y)\n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Senctence:\n",
      " <Start> his favorite favorite is is the banana january but your least favorite is the lime <Pad> <End> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad>\n",
      "\n",
      "True Senctence:\n",
      " <Start> her least favorite fruit is the banana , but your least favorite is the lime . <End> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad>\n",
      "Loss: 0.6782876253128052\n",
      "Predicted Senctence:\n",
      " <Start> the united states is dry may may may , it it is mild warm . . <End> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad>\n",
      "\n",
      "True Senctence:\n",
      " <Start> the united states is nice during may , and it is quiet in august . <End> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad>\n",
      "Loss: 2.227289915084839\n",
      "Predicted Senctence:\n",
      " <Start> california is sometimes warm during june , but it is cold snowy autumn fall . <End> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad>\n",
      "\n",
      "True Senctence:\n",
      " <Start> california is sometimes hot during june , but it is usually snowy in fall . <End> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad>\n",
      "Loss: 0.46210357546806335\n",
      "Predicted Senctence:\n",
      " <Start> the peach is their favorite favorite fruit , but the apple is our least favorite . <End> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad>\n",
      "\n",
      "True Senctence:\n",
      " <Start> the peach is their least favorite fruit , but the apple is our least favorite . <End> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad>\n",
      "Loss: 0.2861543893814087\n",
      "Predicted Senctence:\n",
      " <Start> california is usually dry march may may november is cold relaxing relaxing busy <Pad> <End> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad>\n",
      "\n",
      "True Senctence:\n",
      " <Start> california is usually dry during november , but it is usually chilly in may . <End> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad> <Pad>\n",
      "Loss: 2.253756523132324\n"
     ]
    }
   ],
   "source": [
    "Model.to(\"cpu\")\n",
    "for i in range(5):\n",
    "    Predict_loss(X_test[i],Y_test[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f2161e111f783a6322a6ae262a47844d9386d7dfb61a436620c434d93864cb0f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
